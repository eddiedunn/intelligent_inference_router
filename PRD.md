# Distributed Inference Router System ‚Äì Architecture & MVP Guide

> **For the authoritative MVP/post-MVP feature list, see [IMPLEMENTATION_STATUS.md](../IMPLEMENTATION_STATUS.md).**

## Overview

This architecture enables flexible, cost-efficient LLM inference by routing requests to the most appropriate model or service (local, external, or distributed) using a central **Inference Router**. An *inference router* is a middleware component that dynamically directs incoming AI requests to different models or backends based on factors like task complexity, cost, and resource availability. By intelligently load-balancing and caching requests, the system avoids the throughput and latency issues of naive load balancing in LLM applications. Key goals are to leverage a Mac‚Äôs Apple Silicon (via local models on *MPS*), external APIs (OpenAI, etc.), and a GPU cluster (via **llm-d** on Kubernetes) in one seamless platform. The design draws on best practices from Alibaba‚Äôs LLM router (dynamic scheduling with health metrics), Balaji Balasubramanian‚Äôs inference router principles (cost-aware routing, observability, guardrails), and Red Hat‚Äôs Semantic Router (semantic task routing and caching).

---

## Feature Scope Table

| Feature/Component                | MVP     | Post-MVP         |
|----------------------------------|---------|------------------|
| OpenAI-compatible API            | ‚úÖ      |                  |
| Local Agent (vllm, Docker)       | ‚úÖ      |                  |
| Proxy to OpenAI                  | ‚úÖ      |                  |
| SQLite Model Registry            | ‚úÖ      |                  |
| Docker Compose Dev Stack         | ‚úÖ      |                  |
| CI Workflow                      | ‚úÖ      |                  |
| MkDocs Documentation Site        | ‚úÖ      |                  |
| Redis Caching                    |         | ‚úÖ               |
| Rate Limiting                    |         | ‚úÖ               |
| Smart Routing                    |         | ‚úÖ               |
| Request Logging & Metrics        |         | ‚úÖ               |
| Agent Registration & Heartbeats  |         | ‚úÖ               |
| llm-d Cluster (K8s, Helm)        |         | ‚úÖ               |
| Additional Worker Types (llm-d)  |         | ‚úÖ               |
| Provider Integrations:           |         |                  |
| &nbsp;&nbsp;Anthropic            |         | ‚úÖ               |
| &nbsp;&nbsp;Google               |         | ‚úÖ               |
| &nbsp;&nbsp;OpenRouter           |         | ‚úÖ               |
| &nbsp;&nbsp;Grok                 |         | ‚úÖ               |
| &nbsp;&nbsp;Venice               |         | ‚úÖ               |

**Legend:** ‚úÖ = In MVP, üöß = In progress for MVP, blank = Post-MVP only

---

> **Note:** Only `vllm` (Docker-based) inference workers and OpenAI proxy are supported for MVP. All other worker types and provider integrations are deferred until after MVP.

## Architectural Diagram

&#x20;*Figure: High-level architecture of the distributed inference router system.* The **Router Node** provides an OpenAI-compatible API endpoint (HTTP REST and WebSocket streaming) that clients or the Local Agent can call. Internally, the router uses a **model registry** (in SQLite) to know what models/backends are available and a **Redis cache** to store recent results. Based on the request, the router dispatches inference to one of three backends: (1) a **Local Agent** on Mac for local model execution, (2) an **External API provider** (like OpenAI/Anthropic) via HTTPS proxy, or (3) the **llm-d** inference cluster in Kubernetes for heavy GPU-powered models. This modular design means each component can be developed, run, and tested in isolation ‚Äì e.g. the Local Agent can be run standalone on a Mac for local tests, the Router can be deployed with dummy backends, and llm-d can be exercised independently ‚Äì with clean HTTP/WS interfaces between them.

## Component Responsibilities

### 1. Mac Local Agent (Apple Silicon)

The Local Agent is a lightweight process (native on macOS, not containerized) dedicated to running **local LLM models** on Apple Silicon hardware. It loads one or more smaller models (e.g. 7B-13B parameters) using PyTorch with MPS or other Apple GPU backends. The agent exposes an internal HTTP or WebSocket interface to the Router so it can receive inference requests. On receiving a request (typically a prompt and generation parameters), the agent runs the model locally and returns the completion result. This allows offline or low-latency handling of simple queries on the Mac without involving external services.

* **Interface:** The agent could implement a simple REST endpoint (e.g. `/infer`) or maintain a persistent WebSocket to the router for jobs. It communicates using a standardized format (likely the same OpenAI-like request/response JSON) for simplicity. For streaming token output, a WebSocket or HTTP Server-Sent Events (SSE) stream can be used.
* **Execution:** Uses Python (e.g. PyTorch Lightning or Hugging Face Transformers) to execute the model on the Mac‚Äôs GPU. No container is used ‚Äì the process runs directly on macOS to leverage Metal Performance Shaders (MPS) acceleration.
* **Registration & Health:** On startup, the Local Agent can register its available model(s) with the Router‚Äôs registry (e.g. via an HTTP call) and periodically send heartbeat/health info. This follows the pattern of Alibaba‚Äôs LLM Agent, which registers with the scheduler and reports metrics like queue length and token throughput. In our MVP, registration can be as simple as a static config, but the architecture allows dynamic discovery of agents. If the local agent goes offline or a model error occurs, the Router should mark that backend as unavailable (so it won‚Äôt route traffic there until it recovers).

> **Development:** The Local Agent can be developed and tested independently by invoking it with sample prompts. For example, one could run the agent process on a MacBook and issue HTTP requests to its API to ensure the local model generates outputs correctly. This component focuses purely on local inference; all routing logic is in the Router, keeping the agent simple.

### 2. Router Node (HTTP API & Orchestration)

The Router is the **core orchestrator** of the system. Implemented in Python (e.g. as a FastAPI or Flask app for concurrency), it exposes endpoints that mimic the OpenAI API (such as `/v1/chat/completions`). This means client applications (or the Local Agent) can send requests in the same format as they would to OpenAI, but the Router will handle them internally.

**Key responsibilities:**

* **OpenAI-Compatible API Gateway:** The Router accepts chat completion requests via HTTP (and supports streaming via SSE or WebSocket to the client). This API compatibility allows easy integration with existing tools ‚Äì e.g. one could point the OpenAI Python SDK to the Router‚Äôs URL by configuring the base URL, and it will work transparently. The Router parses incoming JSON (model, prompts, parameters) and authenticates/validates them (for MVP this can be simple or even disabled for localhost testing).

* **Routing Logic:** Based on the requested model or routing policy, the Router delegates the inference to the appropriate backend:

  * *If* **model is designated ‚Äúlocal‚Äù** (e.g. an identifier like `"vicuna-7b-local"`), the Router forwards the request to the Local Agent‚Äôs HTTP endpoint on the Mac, then relays the result back to the client.
  * *If* **model is an external provider** (e.g. `"gpt-4@OpenAI"`), the Router acts as a proxy: it forwards the request to the external API (OpenAI, Anthropic, etc.) using the appropriate client library or HTTP call, waits for the result, and streams or returns it to the caller. The Router will conform to the OpenAI HTTP protocol and streaming format so that clients get the same experience.
  * *If* **model is served in the GPU cluster** (via llm-d), the Router forwards the request to the llm-d cluster‚Äôs endpoint. The interface here can also be HTTP/REST ‚Äì for example, the llm-d service might expose a route for completions that the Router can call (potentially also OpenAI-protocol). In our design we keep this interface simple (JSON in/out over HTTP or a high-throughput RPC). This clean separation means the Router doesn‚Äôt need to know the cluster internals, only how to call its API.

  The selection can also be dynamic. Initially, the Router can require the client to specify which model/provider to use (explicit routing). In a more advanced scenario, the Router could **auto-select a model** based on request content or load: e.g. short/simple prompts go to a small local model, whereas a complex query is sent to a more powerful model. This aligns with the *‚Äúcost savings‚Äù* principle of an inference router ‚Äì using cheaper resources for simple tasks and reserving expensive models for when they‚Äôre truly needed. It also enables specialized skill routing: for instance, math-related questions could be detected and routed to a math-specialized model for better accuracy.

* **Caching Layer:** The Router integrates with **Redis** as a caching layer to store recent responses and even embedding-based indexes for semantic cache. For MVP, this could be a simple cache key of the prompt (or normalized request) to the full completion result. On each request, the Router checks Redis for a cache hit before calling any backend. A cache hit allows it to return the result instantly, reducing latency and saving computation cost. This is especially useful for repeated or semantically similar queries ‚Äì a more advanced version could store vector embeddings of prompts to do semantic similarity lookup (as described by Red Hat‚Äôs semantic router, which caches and recognizes semantically equivalent requests). By conserving back-end calls, caching improves throughput and cost efficiency.

* **Model Registry:** The Router maintains a **registry of available models** and endpoints, stored in a SQLite database (for simplicity and easy local storage). The registry table might include fields like `model_id` (name), `type` (e.g. `local`, `external`, `llm-d`), `endpoint` (URL or identifier), and metadata (max tokens, cost per token for external, etc.). On startup, the Router loads this registry (and it can be updated at runtime to add/remove models). This design comes from the need to decouple routing logic from hard-coded endpoints ‚Äì new models can be registered without code changes. SQLite is lightweight and fits the MVP, though in a scaled deployment a centralized registry service or DB might replace it. (In the **IIR** reference project, SQLite is used similarly for managing model info and configurations.) The Router uses this registry to know where to send each model‚Äôs requests. For instance, `gpt-3.5-turbo` could be flagged as external (OpenAI API), while `vicuna-7b` is local with a URL to the Mac agent, and `flan-xxl` is on llm-d.

* **Metrics & Logging:** Acting as the central gateway, the Router will log and collect metrics about each request. This includes timing (latency of each backend call), success/failure, token counts (if available from model or API response), etc. By aggregating these metrics, the Router provides observability into model performance and usage. For example, one can track how often the local model is used vs external, average response times, or error rates. These metrics can later feed into routing decisions (like if a model is too slow or overloaded, route to an alternative). In MVP, metrics can simply be logged or stored in memory/SQLite; in a production setup, integration with monitoring tools or dashboards would be ideal.

* **Guardrails & Resilience:** The Router can enforce **governance policies** on requests before and after inference. This includes rate limiting clients, input validation (e.g. checking prompt length or content), and post-processing outputs if needed. It can also implement fallbacks ‚Äì for instance, if an external API call fails or times out, the Router might automatically retry or route the request to a backup model (perhaps a simpler local model) to still provide an answer. These are analogous to the *guardrails* described by Balasubramanian. Additionally, the Router itself should be resilient: it can run multiple replicas behind a load balancer (since it‚Äôs stateless aside from cache/DB) to avoid single point of failure. We follow Alibaba‚Äôs router design where the gateway layer (our Router) can failover gracefully ‚Äì e.g. if one Router instance goes down, another takes over, and if the scheduling logic fails, a simpler round-robin routing could temporarily be used. For MVP, a single instance is fine, but we design with failover in mind (e.g. external APIs have error handling, and the system should not hang if one backend is unreachable).

* **Optional Local Inference Mode:** In cases where deploying a separate Local Agent is not desired, the Router itself could load a local model (assuming it‚Äôs running on a machine with an Apple GPU or any GPU) and act as a worker. This ‚Äúembedded local worker‚Äù mode might be used during development (to reduce moving parts) or in a single-machine setup. It sacrifices some modularity but is simpler (just one process to run). The architecture allows toggling this: e.g. the model registry could mark a model as served ‚Äúin-process,‚Äù so the Router knows to handle it internally instead of forwarding externally.

> **Tech Stack:** The Router will be written in Python. FastAPI is a strong choice for the web framework, as it supports asynchronous IO ‚Äì critical for handling many concurrent requests and streaming responses efficiently. FastAPI also makes it easy to define OpenAI-like endpoints and data models. We will use Python HTTP clients for external APIs (or OpenAI‚Äôs SDK), and `redis-py` for cache access. The choice of Python aligns with quick development and leveraging existing AI libraries, although for very high throughput a more performant language or offloading some tasks (as Red Hat did with Rust in their semantic processor) might be considered in the future.

### 3. llm-d GPU Inference Cluster (Kubernetes)

The **llm-d cluster** is a distributed inference service designed for running large-scale models on GPU nodes. It is based on Red Hat‚Äôs open-source **llm-d** project, which extends the vLLM library into a multi-node, high-throughput serving platform. In essence, vLLM provides an efficient transformer inference engine with features like continuous batching and KV-cache optimization, and llm-d makes this scale out across multiple GPUs/machines. This component is where heavy models (that are too large or too slow for the Mac) are hosted, such as 30B+ parameter models or any model requiring specialized hardware.

* **Deployment:** llm-d runs on Kubernetes, which allows it to manage multiple replicas and distribute load. For example, you might deploy one llm-d service per model (or one that can load multiple models) across GPU pods. Each llm-d instance uses vLLM under the hood, taking advantage of *prefill/decode disaggregation* and shared key-value caches to serve many requests efficiently. The cluster is expected to autoscale or at least be horizontally scalable ‚Äì if traffic increases, more pods can be added.

* **Interface:** We define a clear interface between the Router and llm-d. The simplest approach is to treat llm-d as another ‚Äúprovider‚Äù accessible via HTTP. In fact, vLLM can be run as a server that implements the same REST API as OpenAI‚Äôs, making it a drop-in replacement. We can leverage this by configuring llm-d‚Äôs API endpoint and calling it with the same JSON payload as we would to OpenAI (including stream support). Thus, the Router could call `http://llm-d.cluster.local/v1/chat/completions` for a model hosted in the cluster, and stream back the response. Alternatively, a lightweight protocol (gRPC or WebSocket) could be used internally for efficiency. The **contract** is that the Router sends a prompt and generation params to llm-d, and receives either a streaming sequence of tokens or a completed text. The Router doesn‚Äôt need to know the model‚Äôs internals ‚Äì only the model name or ID to request ‚Äì which is configured in the registry mapping to the llm-d endpoint.

* **Coordination & Health:** Similar to the Local Agent, the cluster‚Äôs availability and load should be monitored. In a sophisticated setup, each llm-d instance might run a sidecar agent that reports its health and utilization to the Router (or a separate scheduler service). For MVP, we might poll a `/health` endpoint or handle HTTP errors to detect issues. The router‚Äôs model registry entries for llm-d could include a flag or real-time metrics (e.g. number of busy threads, queue length) if exposed. This allows the Router to perform **load-aware routing** ‚Äì e.g. if model X is deployed on 2 pods, the Router could choose the one with less load or round-robin between them. Alibaba‚Äôs LLM Scheduler concept can be applied here: the router can act as a scheduler that selects the optimal instance based on metrics like queue size or token throughput. In MVP, we can start with a single llm-d service (no choice needed), but the architecture anticipates a more advanced scheduler if scaling up to many instances.

* **Security & Multi-tenancy:** In a production cluster, llm-d might serve multiple applications or tenants. The Router can pass through authentication tokens or use service accounts to securely communicate with it. Kubernetes network policies would ensure only the Router (and authorized clients) can call the llm-d service. While MVP might not include full security, it‚Äôs worth noting that isolation and auth will be important when the cluster is shared or public.

* **Independent Development:** The llm-d service is largely independent ‚Äì it‚Äôs an existing component (likely written in C++/Rust for vLLM and some Go/Rust in Red Hat‚Äôs implementation). Our integration work is to configure it and connect to it. One can deploy llm-d and test it by sending it sample prompts (even using the OpenAI API client as mentioned) to verify it generates correctly. Once verified, the Router simply needs to forward requests to it. This loose coupling ensures that improvements in the llm-d engine (e.g. performance boosts, new models) can be utilized without changes to the Router, aside from updating the registry.

### 4. External LLM Provider Integration

Although not a part of our system per se, external APIs (such as OpenAI, Anthropic, Google PaLM, etc.) are treated as additional ‚Äúbackends‚Äù the Router can route to. The Router holds API keys or endpoints for these providers (configured securely via environment variables or config files). When a request is targeted to an external model (for example, a user might request model `"gpt-4"` which we map to OpenAI‚Äôs API), the Router will format the request according to the provider‚Äôs requirements, add authentication headers, and send it out over HTTPS. The response (e.g. the JSON with the completion and usage tokens from OpenAI) is received by the Router, which then **normalizes it to the common format** we use for the client. Because we standardize on OpenAI‚Äôs schema, this often is just a pass-through of the JSON (possibly filtering out any internal fields).

The Router‚Äôs design allows plugging in new providers easily ‚Äì for instance, adding support for a HuggingFace Inference API or Azure OpenAI would just require implementing a small client module in the Router and adding model entries in the registry. It can also implement **fallback logic**: e.g., if an external provider fails (rate limit or error), the Router could automatically try another provider or a local model, improving reliability.

**Note:** For MVP, one external provider (OpenAI) is sufficient to demonstrate this capability. The focus should be on making sure the Router can handle the slight differences in API (URL, auth, etc.) while maintaining the illusion of a unified API to the user.

## Request Flow Example

To illustrate how these components interact, consider a chat completion request in a fully deployed scenario:

1. **Client -> Router:** A client (could be a UI application or the Mac Local Agent acting on user‚Äôs behalf) sends an HTTP POST to `http://localhost:8000/v1/chat/completions` with a JSON body specifying a model and the conversation prompt. (For example, `model="vicuna-7b-local"` or `"gpt-3.5-turbo"` and a list of messages). This hits the Router‚Äôs FastAPI server.

2. **Router Processing:** The Router authenticates the request if needed (e.g. checks an API key or if it‚Äôs local, it might skip auth). It looks at the `model` field and consults the **model registry** (SQLite) to find how to handle this model.

   * If the model is local, it will have an entry like `endpoint = http://localhost:5000/infer` (assuming the Local Agent runs on port 5000).
   * If it‚Äôs an external model, the entry might say `provider = OpenAI` (and the Router knows to use OpenAI‚Äôs endpoint for that).
   * If it‚Äôs a cluster model, the entry might have `endpoint = http://llm-d-service:8000/v1/chat/completions` (an internal cluster address).

   Suppose the model is a local one. The Router next checks **Redis cache** to see if this exact prompt (or a suitable cache key) was seen recently. If a cached completion is found, it may bypass the backend call and return the cached result immediately (with an indicator that it‚Äôs from cache, if needed). If not cached, the Router proceeds.

3. **Router -> Local Agent:** The Router forwards the request to the Local Agent‚Äôs HTTP API (e.g. POST `http://localhost:5000/infer`) with a payload containing the prompt and parameters (it might convert the OpenAI format into a simpler format the agent expects, if different). This could be done asynchronously so that the Router can handle other requests while waiting. If using WebSockets for streaming, the Router would establish a WS connection to the agent and begin receiving tokens.

4. **Local Agent Inference:** The Local Agent, upon receiving the job, runs the prompt through the loaded model (e.g. a 7B model in 4-bit quantization on the Mac‚Äôs GPU). It generates the completion tokens. In a non-streaming mode, it would compute the full response then send it back in one chunk of JSON. In streaming mode, it would send token by token over the socket/stream. For MVP, returning the full text is simpler; streaming can be added once basics are working.

5. **Local Agent -> Router (Result):** The Router gets the completion result from the agent. It then formats this into the standard response JSON (including entries like `choices`, `usage` if we track tokens, etc., mimicking OpenAI). It also stores the result in Redis cache with the prompt as key (for future semantic or exact matches). The Router logs the time taken and other metrics for this request.

6. **Router -> Client:** Finally, the Router responds to the client‚Äôs HTTP call with the JSON result (or streams it back if applicable). From the client‚Äôs perspective, it just made a call to a ‚Äúlocal OpenAI‚Äù and got a response.

The flow for an external or cluster model is analogous ‚Äì the Router would instead call the external API or the llm-d endpoint in step 3. For an external API, step 4 is handled by the provider‚Äôs server; for llm-d, the cluster does the heavy lifting (possibly distributing the work across GPUs and using optimized KV cache to serve it efficiently). In all cases, the Router unifies the response format.

**Error handling:** If any step fails (e.g. the Local Agent is down, or an external API times out), the Router will catch the exception. It could then either return an error to the client (with an HTTP 500 or an OpenAI-style error message) or try a fallback. For instance, if the cluster is busy/unavailable, the Router might route the request to an external API as a fallback (assuming the output quality is acceptable). These policies can be configured per model. Ensuring the Router never deadlocks and always responds is critical for a robust production system.

## Deployment Scenarios

One benefit of this design is flexibility in deployment. Each component can be deployed according to the environment needs:

* **Development (Standalone Mode):** For an MVP or local demo, everything can run on a single Mac. You would start the Router (Python process) on the Mac itself, along with a Redis instance (e.g. via `brew` or Docker, or even use an in-memory dict for testing cache). You also run the Local Agent on the Mac (another Python process). The Router‚Äôs registry is configured so that the local model points to the agent‚Äôs localhost URL, and external models (if any) have their API keys configured. In this setup, when you `curl localhost:8000/v1/chat/completions`, the Router will handle it and potentially invoke the local agent or external API. This mode requires no Kubernetes or GPUs ‚Äì ideal for developing the Router and Agent logic quickly. (If needed, you could even run a small dummy llm-d server on CPU to simulate the cluster, but it‚Äôs not necessary for MVP.)

* **Distributed (Hybrid Mode):** In a more production-like scenario, you deploy the **Router** in a server or Kubernetes cluster (perhaps as a pod with an attached persistent volume for the SQLite file, and a ConfigMap/Secret for configuration and API keys). The Redis cache can be a separate service (e.g. a Redis cluster or a managed Redis) that the Router pod connects to. The **llm-d** cluster would be deployed on Kubernetes with GPU nodes ‚Äì for example, you might use Helm or operators provided by llm-d/vLLM projects to launch the inference pods. Once llm-d is up, you update the Router‚Äôs model registry to include those models (with the service DNS names or load balancer addresses of llm-d). Now the Router can send traffic to them. The **Local Agent** could still run on a Mac outside the cluster ‚Äì to integrate it, you‚Äôd ensure the Mac is network-accessible to the Router (e.g. via VPN or port-forward). The agent would register itself with the Router (providing its IP/port). In Kubernetes, an alternative is to package a ‚Äúlocal‚Äù worker in a container (if using Linux GPU), but since we specifically want Apple Silicon, the Mac must run it natively. This is an edge/remote node in the architecture. The Router can handle it as just another endpoint.

* **Scaling & High Availability:** In Kubernetes, you can scale the Router horizontally by running multiple replicas behind a Service (load balancer). Since the router is stateless (aside from cache and DB), you‚Äôd use a shared Redis (so all replicas see the same cache) and possibly move the model registry to a network-shared DB or ensure the SQLite is on a shared volume with proper locking. In MVP, we won‚Äôt tackle multi-replica, but the design permits it. The llm-d cluster can scale by adding pods for more throughput, and the Router (or a separate scheduler service) can distribute requests among them based on real-time load metrics. Alibaba‚Äôs approach of a dedicated scheduler is one way: we could run a separate service that all Router instances consult to pick a backend instance. However, a simpler approach is to have the Router itself maintain basic load info and do routing, or leverage Kubernetes services (if llm-d pods behind a service, K8s can round-robin by default).

* **Monitoring & Updates:** Deploying this system in production would include monitoring (logs and metrics from the Router and llm-d). The Router can expose its own metrics endpoint (e.g. Prometheus metrics for requests, latency, etc.), and llm-d likely has metrics for GPU utilization and token throughput. These can feed an autoscaler or just be observed to tune the system. If a new model needs to be added or an endpoint changes, an update to the model registry (and possibly a reload signal to the Router) would propagate that. The modular design means you can update one component without rebuilding everything ‚Äì e.g. upgrade the Local Agent‚Äôs model independently, or update the Router‚Äôs routing rules without touching the cluster.

## MVP Implementation Plan

For the Minimum Viable Product, we will implement a simplified but functional version of the above. The MVP will demonstrate the core routing and inference using one local model, one external API call, and a stub for the cluster. Below is a step-by-step plan:

1. **Router API Skeleton:** Start by implementing the Router‚Äôs HTTP API in Python. Using FastAPI, define an endpoint `POST /v1/chat/completions` that accepts a JSON body with the OpenAI Chat format. For now, it can ignore most fields and just return a hard-coded response (e.g. ‚ÄúHello world‚Äù). Confirm that you can hit this endpoint from a client (e.g. with `curl` or using the OpenAI SDK pointed at `localhost` with an API key configured). This sets up the basic server and request/response handling.

2. **Local Agent Service:** Next, create the Local Agent process. It can also be a FastAPI (or Flask) server or even a simple Python script that listens (for MVP, simplicity is fine ‚Äì e.g. even an HTTP endpoint using Flask). Integrate a small local model: for example, load a 7B parameter model like Llama-2-7B or a distilled variant using Hugging Face Transformers. Ensure the model can run with PyTorch MPS. Provide an endpoint (e.g. `POST /infer`) that takes a prompt and returns a completion text. At first, you can simplify by not handling full chat history ‚Äì just take the latest user message as prompt for the model. Test the agent standalone by posting a prompt and seeing that it returns a coherent completion.

3. **Router-to-Agent Routing:** Connect the Router to the Local Agent. Implement the logic in the Router that if `model` is, say, `"local-7b"`, it will forward the prompt to the agent‚Äôs `/infer` API. You can use `httpx` or `requests` in async mode to call the agent. Parse the agent‚Äôs response and insert it into the Router‚Äôs own response format. Now test end-to-end: run the agent and router, and hit the router‚Äôs `/v1/chat/completions` specifying the local model. The request should flow to the agent and back with a real model-generated answer. This proves local inference routing works.

4. **External API Proxy:** Configure the Router to support an external model (e.g. OpenAI‚Äôs GPT-3.5). Store an API key in the config. Implement a branch in the Router: if `model == "gpt-3.5-turbo"`, format the incoming request to OpenAI‚Äôs API call (which is almost identical JSON). You can use OpenAI‚Äôs Python SDK or direct `requests` to their endpoint. Receive the response and forward it back to the client. This essentially turns the Router into a proxy for external models. Test this by calling the router for `"gpt-3.5-turbo"` and verify it returns a valid completion (meaning the router successfully fetched from OpenAI). This demonstrates the multi-provider capability.

5. **Caching (Redis):** Set up a Redis instance (for dev, you can run it locally or use a Docker container). Integrate a simple caching in the Router: e.g., use the prompt (or last user message) as the key. Before routing a request, check Redis for that key. If present, skip the backend call and return the cached text. If not, after getting a response from backend, store it with an expiry (maybe a few minutes or configurable). This can be done with a few lines using `redis` library. Test that if you send the exact same request twice, the second time the router returns instantly (you can log or measure to confirm). This MVP cache is basic (exact string match), but lays the groundwork for more advanced semantic caching later.

6. **Model Registry (SQLite):** Implement the model registry as a simple SQLite database or even a JSON/YAML file for MVP. Define entries for the models you set up: e.g. one for `"local-7b"` with type ‚Äúlocal‚Äù and the agent URL, one for `"gpt-3.5-turbo"` with type ‚Äúopenai‚Äù and any needed info, and (optionally) a placeholder for a cluster model (e.g. `"flan-xxl"` with type ‚Äúllm-d‚Äù and an endpoint URL that you might mock). Load this registry at router startup. Refactor the routing logic to use this registry lookup instead of hard-coded checks. For example, the router finds the model in registry, sees its `type`, and then calls the corresponding handler (local, external, etc.). This makes it easy to add new models by just editing the registry. As a quick test, try adding a second local model entry pointing to the same agent (if the agent could host multiple models or just to simulate) and see that Router can route to it by name.

7. **Metrics & Logging:** Add logging to the Router for each request: log the model requested, which backend was used, time taken, and token count (if available). You can also keep an in-memory counter or a SQLite table for metrics. For MVP, printing to console or a log file is sufficient. This is important to verify that routing decisions are working (you‚Äôll see in logs if something went to local or external, etc.). It also sets up the habit of collecting data for optimization. For instance, log if a cache hit occurred, or if a fallback was triggered. While a full metrics dashboard is overkill for MVP, ensure the data is there (which helps in debugging and future development).

8. **Testing the MVP:** Finally, exercise the entire system in various scenarios:

   * **Local only:** Disconnect internet (to ensure external calls won‚Äôt work) and call the router for the local model ‚Äì it should return using the Mac‚Äôs model.
   * **External only:** If the local agent is off, call the router for the external model ‚Äì it should still return via OpenAI. (Router should handle the local agent being down; you might simulate this and ensure it times out and returns an error or fallback).
   * **Mixed:** Start both agent and internet, and issue some requests to each model type. Possibly, also test a scenario where the same prompt is asked to the external model and then the local model (to see caching effect ‚Äì though our simple cache might not differentiate by model in MVP, which is something to refine).
   * **Through OpenAI client:** (Optional) Try using the OpenAI Python SDK with `openai.api_base` pointed at your router (`http://localhost:8000/v1`). Set `openai.api_key` to some dummy value that the router will accept (the router can ignore it or you can make it require a specific token). Then call `openai.ChatCompletion.create(model="local-7b", messages=[...])`. This will internally call the router and should work. This is a great end-to-end integration test showing that any OpenAI-compatible client can use the system.

By completing these steps, we will have an MVP that **accepts chat completion requests on localhost** and routes them to either a local Mac model, a remote llm-d (if we set up a dummy endpoint or a real one), or an external API. It will have a rudimentary model registry and caching, and basic metrics logging. From here, we can iteratively enhance each part ‚Äì for example, implement streaming token responses, add more sophisticated routing rules (perhaps using a classification model to choose a backend based on the prompt, as in Red Hat‚Äôs semantic router which uses BERT embeddings), and improve fault tolerance with retries and timeouts.

In summary, this architecture provides a **modular, production-ready blueprint** for distributed LLM inference. The Local Agent, Router, and llm-d cluster can be developed in parallel with clear interfaces: HTTP/WS communication and OpenAI-compatible request formats ensure easy integration. Following the principles of intelligent routing ‚Äì cost optimization, specialized model usage, caching, and guardrails ‚Äì the system can achieve better performance and efficiency than a monolithic approach. The MVP plan focuses on getting a vertical slice working, upon which further capabilities (autoscaling, semantic routing, advanced caching, security, etc.) can be built. With Python as the primary implementation language (for rapid development and leveraging ML libraries) and llm-d/vLLM providing the heavy lifting for distributed GPU inference, this design balances ease of development with scalability for future growth.

**Sources:** Best practices and principles have been incorporated from Alibaba Cloud‚Äôs LLM router design (dynamic scheduling with real-time metrics), Balaji Balasubramanian‚Äôs inference router concept (cost-aware routing, observability, and middleware approach), and Red Hat‚Äôs Semantic Router and llm-d documentation (semantic-based model selection, caching, and distributed vLLM serving). These informed the architecture‚Äôs emphasis on **even load distribution, response caching, model specialization, and robust governance** for a production-ready AI inference system.
