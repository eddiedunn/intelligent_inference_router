# Intelligent Inference Router ‚Äì Feature Checklist

> **For the authoritative MVP/post-MVP feature list, see [IMPLEMENTATION_STATUS.md](../IMPLEMENTATION_STATUS.md).**

---

## üöÄ MVP Features

These features are required for the MVP milestone. All other features are deferred.

### Router
- [x] Build OpenAI-compatible API (`/v1/chat/completions`)
- [x] Connect Router to Local Agent (vllm, Docker-based)
- [x] Proxy External API Calls (OpenAI only)
- [x] Implement SQLite Model Registry

### Local Agent
- [x] Provide Local LLM Service via vLLM (Docker-based only)

### Shared / Infra
- [ ] Docker Compose for Dev Stack
- [ ] Continuous Integration Workflow
- [ ] Documentation Site with MkDocs

### Full Testing
- [ ] Unit Tests
- [ ] Integration Tests

---

## ‚ùå Explicitly NOT in MVP

- [ ] Enable Redis Caching
- [ ] Rate Limiting
- [ ] Smart Routing (intelligent request dispatch)
- [ ] Add Request Logging and Metrics
- [ ] Register Agent with Router
- [ ] Send Periodic Heartbeats
- [x] Forward to llm-d Cluster
- [x] Deploy llm-d via Helm
- [x] Expose Cluster Endpoint to Router
- [ ] Additional Inference Worker Types (llm-d, etc.)
- [ ] Support for additional providers (Anthropic, Google, OpenRouter, Grok, Venice)

---

## üìà Post-MVP Roadmap

Features and integrations planned for after the MVP:

- Caching (e.g., Redis caching)
- Rate limiting
- Smart routing (intelligent request dispatch)
- Request Logging and Metrics
- Agent registration & heartbeats
- llm-d cluster support (forwarding, deployment, endpoint exposure)
- Additional inference worker types (llm-d)
- Provider integrations:
  - Anthropic
  - Google
  - OpenRouter
  - Grok
  - Venice

---

**Note:** Only `vllm` (Docker-based) inference workers and OpenAI proxy are supported for MVP. All other worker types and provider integrations are deferred until after MVP. See [IMPLEMENTATION_STATUS.md](../IMPLEMENTATION_STATUS.md) for the up-to-date status.

so that lightweight prompts run on-device.
**Acceptance criteria:**
- ‚úÖ Router resolves model type `local` via the registry.
- ‚úÖ Requests are sent to `http://<agent-host>:5000/infer` and response relayed.
- ‚úÖ Errors from the agent are surfaced to the client.
**Implementation hints:**
- Use `httpx.AsyncClient` for forwarding.
- Agent URL stored in SQLite registry.
**Test cases:**
- Start agent with sample model and run `curl` through router.
- Unit test with `pytest` mocking agent replies.
**Dependencies/blocks:** Build Local Agent service.

### Proxy External API Calls
**User story:** As a developer I want the router to proxy to OpenAI or Anthropic
so that I can use remote models when needed.
**Acceptance criteria:**
- ‚úÖ Router reads `EXTERNAL_*_KEY` from env to call providers.
- ‚úÖ API requests mirror the OpenAI chat JSON format.
- ‚úÖ Streaming and non-streaming responses work.
**Implementation hints:**
- Use `httpx` or provider SDKs.
- Allow provider base URL via env var.
**Test cases:**
- Mock external call in `pytest` to verify forwarding logic.
- `curl` with real key returns provider output.
**Dependencies/blocks:** Model registry entry for external provider.

### Enable Redis Caching
**MoSCoW:** Should
**User story:** As a frequent caller I want identical prompts cached so that repeat requests return faster.
**Acceptance criteria:**
- ‚úÖ Router checks Redis before forwarding a request.
- ‚úÖ Cached entries expire after a configurable TTL.
- ‚úÖ Cache key includes model name and prompt hash.
**Implementation hints:**
- Use `redis-py` with `REDIS_URL`.
- Add `CACHE_TTL` env var.
**Test cases:**
- `pytest` ensures cache hit skips backend call.
- Observe log message indicating cache usage.
**Dependencies/blocks:** Build Router API.

### Implement SQLite Model Registry
**MoSCoW:** Must
**User story:** As an operator I want a registry of models and endpoints
so that routing rules are configurable without code changes.
**Acceptance criteria:**
- ‚úÖ SQLite DB stores model name, type, and endpoint.
- ‚úÖ Router loads registry on startup.
- ‚úÖ Admin CLI can add or update models.
**Implementation hints:**
- Use `sqlalchemy` ORM with file path from `SQLITE_DB_PATH`.
- Provide `make migrate` and `make seed` commands.
**Test cases:**
- `pytest tests/router/test_registry.py` for CRUD operations.
- Manual check: adding new model allows immediate routing.
**Dependencies/blocks:** Router API skeleton.

### Forward to llm-d Cluster
**MoSCoW:** Could
**User story:** As a user with heavy workloads I want to route to a GPU cluster so that large models run efficiently.
**Acceptance criteria:**
- ‚úÖ Router recognizes model type `llm-d`.
- ‚úÖ Requests hit `${LLMD_ENDPOINT}/v1/chat/completions`.
- ‚úÖ Failures return informative HTTP errors.
**Implementation hints:**
- Configure llm-d via Helm chart in `worker_cluster/chart`.
- Use env var `LLMD_ENDPOINT`.
**Test cases:**
- `curl` through router to cluster returns completion.
- Integration test mocks llm-d endpoint.
**Dependencies/blocks:** Model registry with llm-d entry.

### Add Request Logging and Metrics
**MoSCoW:** Should
**User story:** As an operator I want logs and basic metrics so that routing decisions can be audited and optimized.
**Acceptance criteria:**
- ‚úÖ Each request logs model, backend, latency, and cache hit.
- ‚úÖ Metrics exposed at `/metrics` in Prometheus format.
- ‚úÖ Logs written to stdout and rotated daily.
**Implementation hints:**
- Use Python `logging` module and `prometheus-client`.
- Configure log level via `LOG_LEVEL` env var.
**Test cases:**
- `pytest` asserts metrics counters increment.
- `curl /metrics` returns Prometheus text.
**Dependencies/blocks:** Build Router API and caching.

## Local Agent

### Provide Local LLM Service
**MoSCoW:** Must
**User story:** As a Mac user I want a lightweight HTTP service that runs a local model
so that I can process prompts offline.
**Acceptance criteria:**
- ‚úÖ Agent starts via `python local_agent/main.py` on macOS.
- ‚úÖ `POST /infer` accepts prompt and generation params.
- ‚úÖ Uses PyTorch MPS to load default model `local_mistral-7b-instruct-q4`.
**Implementation hints:**
- Build FastAPI app binding to `:5000`.
- Keep dependencies minimal; rely on `torch` and `transformers`.
**Test cases:**
- `curl localhost:5000/infer` returns text.
- Unit tests mock model for fast execution.
**Dependencies/blocks:** None.

### Register Agent with Router
**MoSCoW:** Should
**User story:** As an operator I want the agent to announce available models
so that the router updates its registry automatically.
**Acceptance criteria:**
- ‚úÖ On startup, agent POSTs model info to `router_host/register`.
- ‚úÖ Router updates SQLite entry or marks agent offline if heartbeat fails.
- ‚úÖ Registration retries on network error.
**Implementation hints:**
- Use `httpx` for registration calls.
- Agent reads router address from `ROUTER_URL` env var.
**Test cases:**
- `pytest` fakes router endpoint and checks registration logic.
- Stop router to verify retry behavior.
**Dependencies/blocks:** Router registry feature.

### Send Periodic Heartbeats
**MoSCoW:** Could
**User story:** As a router maintainer I want health info from agents so that routing avoids failed nodes.
**Acceptance criteria:**
- ‚úÖ Agent sends heartbeat every N seconds with queue length.
- ‚úÖ Router marks agent stale after timeout.
- ‚úÖ Heartbeat interval configurable via env var.
**Implementation hints:**
- Background task with `asyncio.create_task`.
- Use `/heartbeat` endpoint on router.
**Test cases:**
- `pytest` verifies stale agents are pruned.
- Inspect router logs for heartbeat data.
**Dependencies/blocks:** Register Agent with Router.

## GPU Worker Cluster

### Deploy llm-d via Helm
**MoSCoW:** Should
**User story:** As an infra engineer I want a Helm chart for llm-d so that GPU workers can be installed on k3s easily.
**Acceptance criteria:**
- ‚úÖ `make k3s-up` installs llm-d chart under `worker_cluster/chart`.
- ‚úÖ Cluster exposes service `llm-d` on port 8000.
- ‚úÖ Values file allows model selection and GPU resources.
**Implementation hints:**
- Base chart on Red Hat llm-d examples.
- Provide `values.yaml` with minimal defaults.
**Test cases:**
- `helm lint worker_cluster/chart` succeeds.
- Manual `curl` to cluster endpoint returns stub response.
**Dependencies/blocks:** None.

### Expose Cluster Endpoint to Router
**MoSCoW:** Could
**User story:** As a router admin I want llm-d reachable via a stable URL so that requests from the router succeed.
**Acceptance criteria:**
- ‚úÖ Helm chart outputs `LLMD_ENDPOINT` for router config.
- ‚úÖ Service is reachable from router pod or host machine.
- ‚úÖ TLS termination optional but documented.
**Implementation hints:**
- Use Kubernetes Ingress or ClusterIP depending on environment.
- Document connection details in README.
**Test cases:**
- `kubectl get svc llm-d` shows cluster IP.
- Integration test uses env var to hit cluster from router.
**Dependencies/blocks:** Deploy llm-d via Helm.

## Shared / Infra

### Docker Compose for Dev Stack
**MoSCoW:** Should
**User story:** As a contributor I want a single command to start router, Redis, and optional agent
so that local testing is easy.
**Acceptance criteria:**
- ‚úÖ `make docker-dev` launches services via Docker Compose.
- ‚úÖ Env file `.env.example` documents required vars.
- ‚úÖ Stopping the stack cleans up all containers.
**Implementation hints:**
- Compose file includes router image, Redis, and volume for SQLite.
- Mount Mac agent only when running on Darwin.
**Test cases:**
- `docker compose ps` shows all services running.
- E2E pytest uses compose stack for tests.
**Dependencies/blocks:** Router API and Local Agent implementation.

### Continuous Integration Workflow
**MoSCoW:** Should
**User story:** As a project maintainer I want automated lint and test checks
so that PRs stay stable.
**Acceptance criteria:**
- ‚úÖ GitHub Actions runs `make lint` and `make test` on every PR.
- ‚úÖ Docker images build successfully in CI.
- ‚úÖ Failures block merge until resolved.
**Implementation hints:**
- Add `.github/workflows/ci.yml` using the provided make targets.
- Cache Python deps to speed up builds.
**Test cases:**
- Open a dummy PR and verify all checks execute.
**Dependencies/blocks:** Docker Compose for Dev Stack.

### Documentation Site with MkDocs
**MoSCoW:** Could
**User story:** As a new developer I want browsable docs
so that onboarding is quick.
**Acceptance criteria:**
- ‚úÖ `docs/` contains MkDocs config and markdown pages.
- ‚úÖ `make docs-serve` launches local preview on `:8001`.
- ‚úÖ Deployment workflow publishes to GitHub Pages.
**Implementation hints:**
- Base config on `mkdocs-material` theme.
- Include PRD and FEATURES in the nav.
**Test cases:**
- `mkdocs build` runs without errors.
- Preview site loads root page.
**Dependencies/blocks:** Continuous Integration Workflow.

